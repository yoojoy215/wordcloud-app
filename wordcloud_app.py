"""
í•œê¸€ ê¸°ì‚¬ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±ê¸° - Streamlit UI
ì‚¬ìš©ìê°€ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ë©´ ìë™ìœ¼ë¡œ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
from kiwipiepy import Kiwi
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from io import BytesIO

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í•œê¸€ ê¸°ì‚¬ ì›Œë“œ í´ë¼ìš°ë“œ",
    page_icon="ğŸ“°",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
import os

def get_font_path():
    """OSì— ë§ëŠ” í•œê¸€ í°íŠ¸ ê²½ë¡œ ë°˜í™˜"""
    system = platform.system()
    
    if system == 'Windows':
        return 'C:/Windows/Fonts/malgun.ttf'
    elif system == 'Darwin':  # macOS
        return '/System/Library/Fonts/Arial Unicode.ttf'
    else:  # Linux
        possible_paths = [
            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
            '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
        ]
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return None

# í°íŠ¸ ì„¤ì •
font_path = get_font_path()
if font_path and os.path.exists(font_path):
    plt.rcParams['font.family'] = 'DejaVu Sans'
else:
    # í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']

plt.rcParams['axes.unicode_minus'] = False

# ============ í•¨ìˆ˜ ì •ì˜ ============

@st.cache_data
def get_latest_article_from_homepage(homepage_url):
    """ì¤‘ì•™ì¼ë³´ ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ URL ì¶”ì¶œ"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(homepage_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì¤‘ì•™ì¼ë³´ ìµœì‹  ê¸°ì‚¬ ì°¾ê¸°
        if 'joongang' in homepage_url.lower():
            # ë§í¬ì—ì„œ /article/ íŒ¨í„´ ì°¾ê¸°
            article_links = soup.find_all('a', href=True)
            for link in article_links:
                href = link.get('href', '')
                if '/article/' in href and href.startswith('/'):
                    article_url = f"https://www.joongang.co.kr{href}"
                    return article_url
                elif '/article/' in href and href.startswith('http'):
                    return href
        
        # ì¡°ì„ ì¼ë³´
        elif 'chosun' in homepage_url.lower():
            article_links = soup.find_all('a', href=True)
            for link in article_links:
                href = link.get('href', '')
                if '/article/' in href and href.startswith('http'):
                    return href
        
        # ë™ì•„ì¼ë³´
        elif 'donga' in homepage_url.lower():
            article_links = soup.find_all('a', href=True)
            for link in article_links:
                href = link.get('href', '')
                if '/article/' in href:
                    if href.startswith('http'):
                        return href
                    elif href.startswith('/'):
                        return f"https://www.donga.com{href}"
        
        return None
        
    except Exception as e:
        return None

def normalize_url(url):
    """URL ì •ê·œí™” - ë©”ì¸ í˜ì´ì§€ë©´ ìµœì‹  ê¸°ì‚¬ë¡œ ë³€í™˜"""
    url = url.strip()
    
    # ë©”ì¸ í˜ì´ì§€ íŒ¨í„´ ê°ì§€
    is_homepage = (
        url.endswith('/') or 
        url.endswith('joongang.co.kr') or 
        url.endswith('chosun.com') or
        url.endswith('donga.com') or
        'www.joongang.co.kr' in url and '/article/' not in url
    )
    
    if is_homepage:
        # ìµœì‹  ê¸°ì‚¬ ì¶”ì¶œ
        article_url = get_latest_article_from_homepage(url if url.startswith('http') else f"https://{url}")
        if article_url:
            return article_url, "ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ë¡œ ìë™ ë³€í™˜ë¨"
        else:
            return url, "ìµœì‹  ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ê¸°ì‚¬ URLì„ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    return url, None

@st.cache_data
def scrape_article(url):
    """ê¸°ì‚¬ í¬ë¡¤ë§"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
        title_elem = (soup.find('h1', class_='headline') or 
                     soup.find('h1', class_='title') or
                     soup.find('h1', class_='article_title') or
                     soup.find('h2', class_='title') or
                     soup.find('h1') or
                     soup.find('title'))
        title = title_elem.text.strip() if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
        article_body = (soup.find('div', {'id': 'article_body'}) or
                       soup.find('div', class_='article_body') or
                       soup.find('div', class_='article-body') or
                       soup.find('div', class_='article_text') or
                       soup.find('div', class_='content_body') or
                       soup.find('article') or
                       soup.find('div', class_='content'))
        
        if article_body:
            paragraphs = article_body.find_all('p')
            content = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])
        else:
            # ëª¨ë“  p íƒœê·¸ ìˆ˜ì§‘ (100ì ì´ìƒë§Œ)
            paragraphs = soup.find_all('p')
            content = ' '.join([p.text.strip() for p in paragraphs if len(p.text.strip()) > 50])
        
        # ë‚´ìš©ì´ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬
        if not content or len(content) < 100:
            return title, "error: ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°œë³„ ê¸°ì‚¬ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ë©”ì¸ í˜ì´ì§€ ì œì™¸)"
        
        return title, content
        
    except requests.exceptions.Timeout:
        return None, "error: ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (ì—°ê²° ë¬¸ì œ)"
    except requests.exceptions.ConnectionError:
        return None, "error: ì—°ê²° ì‹¤íŒ¨"
    except Exception as e:
        return None, f"error: {str(e)}"

@st.cache_resource
def get_kiwi():
    """Kiwi ì´ˆê¸°í™” (ìºì‹±)"""
    return Kiwi()

def analyze_morphemes(text, top_n=50):
    """í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬ ì¶”ì¶œ"""
    kiwi = get_kiwi()
    
    # í˜•íƒœì†Œ ë¶„ì„
    result = kiwi.analyze(text)
    
    # ëª…ì‚¬ë§Œ ì¶”ì¶œ
    nouns = []
    for token_result in result:
        for morph in token_result[0]:
            if morph.tag in ['NNG', 'NNP']:
                if len(morph.form) > 1:
                    nouns.append(morph.form)
    
    # ë¹ˆë„ ê³„ì‚°
    noun_counts = Counter(nouns)
    
    return noun_counts

def remove_stopwords(noun_counts, custom_stopwords=None):
    """ë¶ˆìš©ì–´ ì œê±°"""
    default_stopwords = [
        'ê¸°ì', 'ë‰´ìŠ¤', 'ì‚¬ì§„', 'ì—°í•©ë‰´ìŠ¤', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´',
        'ì¡°ì„ ì¼ë³´', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'ì´ë°ì¼ë¦¬', 'ë‰´ì‹œìŠ¤',
        'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„',
        'ëŒ€í•œë¯¼êµ­', 'ì„œìš¸', 'í•œêµ­', 'ìš°ë¦¬', 'ì €í¬', 'ê´€ë ¨',
        'ì œê³µ', 'ë¬´ë‹¨', 'ì „ì¬', 'ì¬ë°°í¬', 'ê¸ˆì§€', 'ì €ì‘ê¶Œì'
    ]
    
    if custom_stopwords:
        default_stopwords.extend(custom_stopwords)
    
    filtered_nouns = {
        word: count for word, count in noun_counts.items()
        if word not in default_stopwords
    }
    
    return filtered_nouns

def create_wordcloud(word_freq, width=1200, height=800, colormap='viridis'):
    """ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±"""
    if not word_freq:
        return None
    
    font_path_to_use = get_font_path()
    
    wc = WordCloud(
        font_path=font_path_to_use if font_path_to_use and os.path.exists(font_path_to_use) else None,
        width=width,
        height=height,
        background_color='white',
        max_words=100,
        relative_scaling=0.3,
        colormap=colormap
    ).generate_from_frequencies(word_freq)
    
    return wc

# ============ UI êµ¬ì„± ============

# í—¤ë”
st.title("ğŸ“° í•œê¸€ ê¸°ì‚¬ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±ê¸°")
st.markdown("---")
st.markdown("""
### ì‚¬ìš© ë°©ë²•
1. ì¤‘ì•™ì¼ë³´ ë©”ì¸ í˜ì´ì§€ ë˜ëŠ” ê°œë³„ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”
2. 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
3. ì›Œë“œ í´ë¼ìš°ë“œê°€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤!

**ì§€ì› ì‚¬ì´íŠ¸:** ì¤‘ì•™ì¼ë³´, ì¡°ì„ ì¼ë³´, ë™ì•„ì¼ë³´, í•œê²¨ë ˆ, ê²½í–¥ì‹ ë¬¸ ë“± ëŒ€ë¶€ë¶„ì˜ í•œê¸€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸

âœ¨ **íŠ¹ì§•:** ë©”ì¸ í˜ì´ì§€(https://www.joongang.co.kr/)ë¥¼ ì…ë ¥í•˜ë©´ ìµœì‹  ê¸°ì‚¬ë¡œ ìë™ ë³€í™˜ë©ë‹ˆë‹¤!
""")

# ì‚¬ì´ë“œë°” - ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # URL ì…ë ¥
    default_url = "https://www.joongang.co.kr/" 
    article_url = st.text_input(
        "ê¸°ì‚¬ URL",
        value=default_url,
        placeholder="https://..."
    )
    
    st.caption("ğŸ’¡ íŒ: ì¤‘ì•™ì¼ë³´ ë©”ì¸ í˜ì´ì§€ë¥¼ ì…ë ¥í•˜ë©´ ìµœì‹  ê¸°ì‚¬ë¡œ ìë™ ë³€í™˜ë©ë‹ˆë‹¤!")
    
    # ê³ ê¸‰ ì„¤ì •
    st.subheader("ê³ ê¸‰ ì„¤ì •")
    
    top_n = st.slider("í‘œì‹œí•  ë‹¨ì–´ ê°œìˆ˜", 10, 100, 50)
    
    custom_stopwords_input = st.text_area(
        "ì¶”ê°€ ë¶ˆìš©ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        placeholder="ì˜ˆ: ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3"
    )
    
    custom_stopwords = [w.strip() for w in custom_stopwords_input.split(',') if w.strip()]
    
    colormap = st.selectbox(
        "ìƒ‰ìƒ í…Œë§ˆ",
        ['viridis', 'plasma', 'inferno', 'magma', 'cool', 'hot', 'spring', 'summer', 'autumn', 'winter']
    )
    
    analyze_button = st.button("ğŸ” ë¶„ì„ ì‹œì‘", use_container_width=True, type="primary")

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([1, 1])

if analyze_button:
    if not article_url:
    if not article_url:
        st.error("âŒ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        # URL ì •ê·œí™” (ë©”ì¸ í˜ì´ì§€ë©´ ìµœì‹  ê¸°ì‚¬ë¡œ ë³€í™˜)
        with st.spinner('URL í™•ì¸ ì¤‘...'):
            normalized_url, conversion_msg = normalize_url(article_url)
        
        if conversion_msg:
            if "ìë™ ë³€í™˜" in conversion_msg:
                st.info(f"âœ… {conversion_msg}")
            else:
                st.warning(f"âš ï¸ {conversion_msg}")
        
        if normalized_url != article_url:
            article_url = normalized_url
        
        # ê¸°ì‚¬ í¬ë¡¤ë§
        with st.spinner('ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...'):
            title, content = scrape_article(article_url)
        
        if content and not content.startswith("error"):
            # ê¸°ì‚¬ ì •ë³´ í‘œì‹œ
            with col1:
                st.subheader("ğŸ“„ ê¸°ì‚¬ ì •ë³´")
                st.markdown(f"**ì œëª©:** {title}")
                st.markdown(f"**ë³¸ë¬¸ ê¸¸ì´:** {len(content):,}ì")
                
                with st.expander("ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°"):
                    st.text(content[:500] + "..." if len(content) > 500 else content)
            
            # í˜•íƒœì†Œ ë¶„ì„
            with st.spinner('í˜•íƒœì†Œ ë¶„ì„ ì¤‘...'):
                noun_counts = analyze_morphemes(content, top_n)
                filtered_nouns = remove_stopwords(noun_counts, custom_stopwords)
            
            # ìƒìœ„ ëª…ì‚¬ í‘œì‹œ
            with col2:
                st.subheader(f"ğŸ“Š ìƒìœ„ {min(top_n, len(filtered_nouns))}ê°œ ëª…ì‚¬")
                
                top_nouns = dict(sorted(filtered_nouns.items(), key=lambda x: x[1], reverse=True)[:top_n])
                
                # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
                import pandas as pd
                df_nouns = pd.DataFrame(list(top_nouns.items()), columns=['ëª…ì‚¬', 'ë¹ˆë„'])
                df_nouns.index = range(1, len(df_nouns) + 1)
                st.dataframe(df_nouns, use_container_width=True, height=400)
            
            # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
            st.markdown("---")
            st.subheader("â˜ï¸ ì›Œë“œ í´ë¼ìš°ë“œ")
            
            with st.spinner('ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘...'):
                # colormap ì—…ë°ì´íŠ¸
                wc = create_wordcloud(filtered_nouns, width=1400, height=700, colormap=colormap)
                
                if wc:
                    fig, ax = plt.subplots(figsize=(16, 8))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis('off')
                    ax.set_title(f'ì›Œë“œ í´ë¼ìš°ë“œ: {title}', fontsize=16, fontweight='bold', pad=20)
                    plt.tight_layout()
                    
                    st.pyplot(fig)
                    
                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                    buf = BytesIO()
                    fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
                    buf.seek(0)
                    
                    st.download_button(
                        label="ğŸ’¾ ì›Œë“œ í´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ",
                        data=buf,
                        file_name=f"wordcloud_{title[:20]}.png",
                        mime="image/png",
                        use_container_width=True
                    )
                else:
                    st.error("ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
            
        else:
            error_msg = content if content else 'ì•Œ ìˆ˜ ì—†ìŒ'
            if "error:" in error_msg:
                error_msg = error_msg.replace("error: ", "")
            st.error(f"âŒ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ“ {error_msg}\n\nğŸ’¡ ê°œë³„ ê¸°ì‚¬ URLì„ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. (ë©”ì¸ í˜ì´ì§€ ì œì™¸)")

else:
    # ì´ˆê¸° í™”ë©´
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ URLì„ ì…ë ¥í•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    
    # ì˜ˆì‹œ ì´ë¯¸ì§€ë‚˜ ì„¤ëª…
    st.markdown("""
    ### âœ¨ ì£¼ìš” ê¸°ëŠ¥
    
    - ğŸŒ **ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì§€ì›**: ì¤‘ì•™ì¼ë³´, ì¡°ì„ ì¼ë³´, ë™ì•„ì¼ë³´, í•œê²¨ë ˆ ë“± ëŒ€ë¶€ë¶„ì˜ í•œê¸€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
    - ğŸ” **ì •í™•í•œ í˜•íƒœì†Œ ë¶„ì„**: Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
    - ğŸ¨ **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ìƒ‰ìƒ, ë‹¨ì–´ ê°œìˆ˜, ë¶ˆìš©ì–´ ì„¤ì • ê°€ëŠ¥
    - ğŸ’¾ **ë‹¤ìš´ë¡œë“œ**: ê³ í•´ìƒë„ PNGë¡œ ì €ì¥
    
    ### ğŸ“Œ íŒ
    - **ê°œë³„ ê¸°ì‚¬ URL**ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤ (ë©”ì¸ í˜ì´ì§€ X)
    - ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ë¶ˆìš©ì–´ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”
    - ë‹¤ì–‘í•œ ìƒ‰ìƒ í…Œë§ˆë¥¼ ì‹œë„í•´ë³´ì„¸ìš”
    - ë‹¨ì–´ ê°œìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ ì›í•˜ëŠ” ìˆ˜ì¤€ì˜ ìƒì„¸ë„ë¥¼ ì–»ìœ¼ì„¸ìš”
    
    ### ğŸ”— ê¸°ì‚¬ URL ì˜ˆì‹œ
    - âœ… https://www.joongang.co.kr/article/25398258
    - âœ… https://www.chosun.com/article/60012345
    - âŒ https://www.joongang.co.kr/ (ë©”ì¸ í˜ì´ì§€)
    """)

# í‘¸í„°
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Made with â¤ï¸ using Streamlit & Kiwi</p>
</div>
""", unsafe_allow_html=True)